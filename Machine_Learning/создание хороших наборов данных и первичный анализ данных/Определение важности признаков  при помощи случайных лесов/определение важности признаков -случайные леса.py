

"""
    В силу вышесказанного существуют два общих подхода к приведению разных
    признаков к одинаковой шкале: нормализация и стандартизация. В различных об­
    ластях эти термины нередко используются довольно нечетко, и их конкретное со­
    держание приходится выводить из контекста. Чаще всего нормализация означает
    приведение (нормирование) признаков к диапазону [О, 1]  и является частным слу­
    чаем минимаксного масштабирования.  Для нормализации наших данных можно
    к каждому признаковому столбцу просто применить минимаксное масштабирова­
    ние, где новое значение x~2rm из образца x<i)


    Процедура минимаксного масштабирования реализована в  библиотеке scikit-
    leaш и может быть применена следующим образом:
    from  sklearn.preprocessing  import  MinMaxScaler
    mms =MinMaxScaler()
    X_train_norm=mms.fit_transform(X_train)
    X_test_norm=mms.transform(X_test)
    ===========================================================
    При помощи стандартизации мы центрируем признаковые столбцы
    в нулевом среднем значении, т. е. равном О, с единичным стандартным отклонением,
    т. е. равным 1, в результате чего признаковые столбцы принимают вид нормального
    распределения, что упрощает извлечение весов.  Кроме того, стандартизация содер­
    жит полезную информацию о выбросах и делает алгоритм менее к ним чувстви­
    тельным, в отличие от минимаксного масштабирования, которое шкалирует данные
    в ограниченном диапазоне значений.

    Помимо класса  минимаксного  масштабирования  MinMaxScaler,  в  библиотеке
    scikit-leaгn также реализован класс стандартизации StandardScaler:
    from  sklearn . preprocessing  i mport  StandardScaler
    stdsc  =  StandardScaler ()
    X_ train _std  =  stdsc .fit _ trans f orm (X_  t r ain)
    X_tes t_std =  stdsc. transform(X_tes t)

    """

import pandas as pd
import numpy as np

url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'

df_wine = pd.read_csv(url, header=None)  # считываем набор данных вин для проведения исследования

df_wine.columns = ['Метка класса', 'Алкоголь',
                       'Яблочная кислота', 'Зола',
                       'Щелочность золы', 'Магний',
                       'Всего фенола', 'Флаваноиды',
                       'Фенолы нефлаваноидные', 'Проантоцианины',
                       'Интенсивность цвета', 'Оттенок',
                       'OD280/OD315 разбавленныех вин', 'Пролин']
print('Метки классов:', np.unique(df_wine['Метка класса']))
df_wine.head()

"""
    Функция train_test_split из подмодуля отбора модели model_selection  библиоте­
    ки scikit-leaгn (в версии библиотеки< 0.18 этот модуль назывался cross_validation) 
    предоставляет удобный способ случайным образом разделить этот набор данных на 
    отдельные тестовый и тренировочный наборы данных: 
    """

from sklearn.model_selection import train_test_split

"""
    Сначала мы присвоили переменной Х представленные в виде массива NumPy 
    признаковые столбцы 1-13, а переменной у присвоили метки классов из первого 
    столбца. Затем мы применили функцию train_test_split,  чтобы случайным образом 
    разделить Х и у на отдельные тренировочный и тестовый наборы данных.  Установив 
    параметр test_size=O.З, мы присвоили 30%  образцов вин массивам X_test  и y_test 
    и оставшиеся 70% образцов - соответственно массивам X_train и y_train. 

    """
X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values
X_train, X_test, y_train, y_test = \
    train_test_split(X, y, test_size=0.3, random_state=0)

#===============================================================================================

"""
Используя случайный
лес, можно измерить важность признака как усредненное уменьшение неоднород­
ности, вычисленное из всех деревьев решений в лесе, не делая никаких допущений
по поводу наличия либо отсутствия линейной разделимости данных. Удобно то, что
в реализации случайных лесов в библиотеке scikit-learn уже предусмотрено акку­
мулирование важностей признаков, и к ним можно обращаться через атрибут fea-
ture_importances_ после подгонки классификатора RandomForestClassifier. Выполнив
приведенный ниже исходный код, натренируем лес из 1 О ООО деревьев на наборе
данных сортов вин и упорядочим 13 признаков по их соответствующим мерам важ­
ности.

"""
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
feat_labels=df_wine.columns[1:]
forest=RandomForestClassifier(n_estimators=10000,
                              random_state=0,n_jobs=-1)
forest.fit(X_train,y_train)
importances=forest.feature_importances_
indices=np.argsort(importances)[::-1]
for f in range(X_train.shape[1]):
    print("%2d) %-*s %f" %(f+1,30,feat_labels[indices[f]],importances[indices[f]]))


plt.title('Важности признаков')
plt.bar(range(X_train.shape[1]),importances[indices],
        color='lightblue',align='center')

plt.xticks(range(X_train.shape[1]),feat_labels[indices],rotation=90)
plt.xlim([-1,X_train.shape[1]])
plt.tight_layout()
plt.show()



