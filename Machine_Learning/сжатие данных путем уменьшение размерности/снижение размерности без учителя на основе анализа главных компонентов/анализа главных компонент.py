

"""
Анализ главных компонент (pгincipal component analysis,  РСА) - это метод ли­
нейного преобразования, относящийся к типу обучения без учителя, который ши­
роко используется в самых разных областях, чаще всего для снижения размерности.

"""

#==================================Общая и объясненная дисперсия ==============================
"""
Сначала начнем с загрузки набора данных сортов вин,
"""

#=================================Загрузка данных===============================================
import pandas as pd
import numpy as np

#===================================================================

from matplotlib.colors import ListedColormap
def plot_description_regions(X,y,classifier,test_idx=None,resolution=0.02):
    """реализуем небольшую вспомогательную функцию , которая визуально паказывает
    границы решений для двумерных наборов  данных"""

    #настроить генератор маркеров и палитру
    markers=('s','x','o','^','v')   #определяем маркера
    colors=('red','blue','lightgreen','gray','cyan')    #определяем цвета
    cmap=ListedColormap(colors[:len((np.unique(y)))])   #создаем палитрку

    #вывести поверхность решения
    x1_min,x1_max=X[:,0].min()-1,X[:,0].max()+1   #определяем мин и макс для 2 признаков
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1,xx2=np.meshgrid(np.arange(x1_min,x1_max,resolution),
                        np.arange(x2_min,x2_max,resolution))    #создаем пару матричных массивов на основе получившихся признаков
    Z=classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)
    Z=Z.reshape(xx1.shape)
    plt.contourf(xx1,xx2,Z,alpha=0.4,cmap=cmap)
    plt.xlim(xx1.min(),xx1.max())
    plt.ylim(xx2.min(),xx2.max())

    #Показать Все образцы
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y==cl,0],y=X[y==cl,1],
                    alpha=0.8,c=cmap(idx),
                    marker=markers[idx],label=cl)
        #Выделить тестовые образцы
        if test_idx:
            X_test,Y_test=X[test_idx,:],y[test_idx]
            plt.scatter(X_test[:,0],X_test[:,1],c='',
                        alpha=1.0,linewidths=3,marker='o',
                        s=55,label='Тестовый набор')

#===================================================================


url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'

df_wine = pd.read_csv(url, header=None)  # считываем набор данных вин для проведения исследования

df_wine.columns = ['Метка класса', 'Алкоголь',
                       'Яблочная кислота', 'Зола',
                       'Щелочность золы', 'Магний',
                       'Всего фенола', 'Флаваноиды',
                       'Фенолы нефлаваноидные', 'Проантоцианины',
                       'Интенсивность цвета', 'Оттенок',
                       'OD280/OD315 разбавленныех вин', 'Пролин']
print('Метки классов:', np.unique(df_wine['Метка класса']))
df_wine.head()

"""
    Функция train_test_split из подмодуля отбора модели model_selection  библиоте­
    ки scikit-leaгn (в версии библиотеки< 0.18 этот модуль назывался cross_validation) 
    предоставляет удобный способ случайным образом разделить этот набор данных на 
    отдельные тестовый и тренировочный наборы данных: 
    """

from sklearn.model_selection import train_test_split

"""
    Сначала мы присвоили переменной Х представленные в виде массива NumPy 
    признаковые столбцы 1-13, а переменной у присвоили метки классов из первого 
    столбца. Затем мы применили функцию train_test_split,  чтобы случайным образом 
    разделить Х и у на отдельные тренировочный и тестовый наборы данных.  Установив 
    параметр test_size=O.З, мы присвоили 30%  образцов вин массивам X_test  и y_test 
    и оставшиеся 70% образцов - соответственно массивам X_train и y_train. 

"""
X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values
X_train, X_test, y_train, y_test = \
        train_test_split(X, y, test_size=0.3, random_state=0)

# ===================================Нормализация данных===============================
from  sklearn.preprocessing import MinMaxScaler

mms = MinMaxScaler()
X_train_norm = mms.fit_transform(X_train)
X_test_norm = mms.transform(X_test)
print(X_test_norm)
print('----------------------------------------------')
print(X_train_norm)
print('####################################################################################')
# ======================================Стандартизация===============================
from  sklearn.preprocessing import StandardScaler

stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
X_test_std = stdsc.transform(X_test)

print(X_test_std)
print('----------------------------------------------')
print(X_train_std)
print('####################################################################################')
#=================================Конец загрузки данных=========================================
"""
После завершения обязательных шагов предобработки, содержащихся в приве­
денном выше фрагменте исходного кода, перейдем ко второму шагу: построению 
ковариационной матрицы. В симметричной ковариационной dхd-матрице, где d -
число размерностей в наборе данных, хранятся попарные ковариации (согласован­
ные отклонения) между разными признаками.
"""

#===============================Построение коовариационной матрицы================================
"""
Поскольку вычисление собствен­
ных векторов и собственных значений ручным способом является несколько утоми­
тельной и довольно трудоемкой задачей, для получения собственных пар ковариа­
ционной матрицы сортов вин мы воспользуемся функцией linalg.eig из библиотеки 
NumPy: 
"""
"""
При помощи функции numpy.cov мы вычислили ковариационную матрицу стан­
дартизированного тренировочного набора данных. А при помощи функции linalg. 
eig, которая вернула вектор eigen_vals, состоящий из 13 собственных значений и со­
ответствующих собственных векторов, хранящихся как столбцы в 1Зх 13-матрице 
eigen_vecs, выполнили разложение по собственным значениям. 

"""
import numpy as np
cov_matrix=np.cov(X_train_std.T)
eigen_vals,eigen_vecs=np.linalg.eig(cov_matrix)
print('\n Собственные значения \n%s' %eigen_vals)
#==================================================

"""
Тогда при помощи функции cumsum библиотеки NumPy можно вычислить ку­
мулятивную сумму объясненных дисперсий, которую затем можно отобразить на 
графике, воспользовавшись функцией step библиотеки matplotlib: 
"""
tot=sum(eigen_vals)
var_exp=[(i/tot) for i in sorted(eigen_vals,reverse=True)]
cum_var_exp=np.cumsum(var_exp)
import matplotlib.pyplot as plt
plt.bar(range(1,14),var_exp,alpha=0.5,align='center',
        label='индивидуальная объясненная дисперсия')
plt.step(range(1,14),cum_var_exp,where='mid',label='кумулятивная объясненная дисперсия')
plt.ylabel('доля объясненной дисперсии')
plt.xlabel('Главные компоненты')
plt.legend(loc='best')
plt.show()
#==============================Преобразование признаков=========================
"""
После того как мы успешно разложили ковариационную матрицу на собственные 
пары, теперь перейдем к последними трем шагам, чтобы преобразовать набор дан­
ных сортов вин на новые оси главных компонент.  В этом разделе мы отсортируем 
собственные пары в порядке убывания собственных значений, построим проекци­
онную матрицу из отобранных собственных векторов и воспользуемся проекцион­
ной матрицей для преобразования данных в подпространство более низкой размер­
ности. 
Начнем с сортировки собственных пар в порядке убывания собственных значений:
"""
eigen_pairs=[(np.abs(eigen_vals[i]),eigen_vecs[:,i])
             for i in range(len(eigen_vals))]

eigen_pairs.sort(reverse=True)
"""
Затем возьмем два собственных вектора, соответствующих двум самым большим 
значениям, которые захватывают примерно 60% дисперсии в этом наборе данных. 
Отметим, что мы отобрали всего два собственных вектора в целях иллюстрации, 
поскольку позже в этом подразделе собираемся отобразить данные на двумерном 
точечном графике (диаграмме рассеяния).  На практике число главных компонент 
следует определять, исходя из компромисса между вычислительной эффективно­
стью и качеством классификатора: 

"""

w=np.hstack((eigen_pairs[0][1][:,np.newaxis],eigen_pairs[1][1][:,np.newaxis]))
print('Матрица W:\n',w)

"""
Выполнив  приведенный  выше  фрагмент  кода,  мы  создали  проекционную 
13Х2-матрицу W из двух верхних собственных векторов. При помощи проекцион­
ной матрицы теперь можно преобразовать образец х (представленную как вектор­
строка размера 13х 1) на подпространство РСА,  получив теперь уже 2-мерный век­
тор х' образца, состоящий из двух новых признаков: 
"""

print(X_train_std[0].dot(w))

X_train_pca=X_train_std.dot(w)
"""
Наконец,  представим преобразованный тренировочный набор сортов вин, теперь 
уже хранящийся в виде 124Х2-матрицы, на двумерном точечном графике: 

"""

colors=['r','b','g']
markers=['s','x','o']
for l,c,m in zip(np.unique(y_train),colors,markers):
    plt.scatter(X_train_pca[y_train==l,0],
                X_train_pca[y_train==l,1],
                c=c,label=l,marker=m)

plt.xlabel('PC 1')
plt.xlabel('PC 2')
plt.legend(loc='lower left')
plt.show()

"""
Как видно на итоговом графике, данные более разбросаны вдоль оси Х - первая 
главная компонента, чем вдоль второй главной компоненты (ось У), что согласуется 
с графиком долей объясненной дисперсии, который мы построили в предыдущем 
подразделе. Вместе с тем интуитивно можно отметить, что линейный классифика­
тор,  вероятно, сможет хорошо разделить классы: 

"""







#==========================================Анализ главных компонентов ====================================================
"""
Класс РСА - это еще один класс­
преобразователь, в котором мы сначала выполняем подгонку модели, используя для 
этого тренировочные данные, прежде чем беремся за преобразование тренировоч­
ных и тестовых данных с использованием тех же самых параметров модели. Теперь 
применим класс РСА библиотеки scikit-leaгn к тренировочному набору данных сор­
тов вин, выполним классификацию преобразованных образцов посредством логис­
тической регрессии и представим области решений на графике при помощи функ­
ции plot_decision_region
"""

from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
pca=PCA(n_components=2)
lr=LogisticRegression()
X_train_pca=pca.fit_transform(X_train_std)
X_test_pca=pca.transform(X_test_std)
lr.fit(X_train_pca,y_train)
plot_description_regions(X_train_pca,y_train,classifier=lr)
plt.xlabel('PC1')
plt.ylabel('PCA2')
plt.legend(loc='lower left')
plt.show()


"""
Ради полноты построим график областей 
решения логистической регрессии на преобразованном тестовом наборе данных, 
чтобы увидеть, сможет ли она хорошо разделить классы: 

"""

plot_description_regions(X_test_pca,y_test,classifier=lr)
plt.xlabel('PC1')
plt.ylabel('PCA2')
plt.legend(loc='lower left')
plt.show()









#======================================================================================